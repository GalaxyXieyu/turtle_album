name: Build and Deploy

on:
  push:
    branches: [main]
  workflow_dispatch:

env:
  IMAGE_NAME: ${{ vars.IMAGE_NAME || 'turtle_album' }}
  K8S_NAMESPACE: ${{ vars.K8S_NAMESPACE || 'ns-lmgpb9nc' }}
  K8S_WORKLOAD_NAME: ${{ vars.K8S_WORKLOAD_NAME || 'turtles-album' }}
  K8S_WORKLOAD_KIND: ${{ vars.K8S_WORKLOAD_KIND || 'auto' }}
  K8S_CONTAINER_NAME: ${{ vars.K8S_CONTAINER_NAME || '' }}
  REQUIRE_DATA_PVC: ${{ vars.REQUIRE_DATA_PVC || 'true' }}

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Checkout api-auto-test framework
        uses: actions/checkout@v4
        with:
          repository: GalaxyXieyu/Api-Test-MCP
          path: api-auto-test
          # 为空会导致 actions/checkout 报错；优先用仓库 Secret（支持私有仓库），否则回退到默认 GITHUB_TOKEN
          token: ${{ secrets.GH_TOKEN || github.token }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install backend dependencies (isolated venv)
        run: |
          python -m venv .venv-backend
          .venv-backend/bin/python -m pip install --upgrade pip
          .venv-backend/bin/pip install -r backend/requirements.txt

      - name: Install test framework (isolated venv)
        run: |
          python -m venv .venv-tests
          .venv-tests/bin/python -m pip install --upgrade pip
          .venv-tests/bin/pip install pytest allure-pytest pyyaml requests httpx
          .venv-tests/bin/pip install -e ./api-auto-test

      - name: Start API server
        run: |
          cd backend
          timeout 30s bash -c '../.venv-backend/bin/uvicorn app.main:app --host 0.0.0.0 --port 8000 &' 

          # Startup can include Alembic migrations; poll /health instead of fixed sleep.
          for i in $(seq 1 30); do
            if curl -fsS http://localhost:8000/health >/dev/null; then
              echo "API is healthy"
              exit 0
            fi
            sleep 1
          done

          echo "::error::API did not become healthy in time"
          curl -s http://localhost:8000/health || true
          exit 1

      - name: Run API tests
        run: |
          cd backend
          ../.venv-tests/bin/python -m pytest tests/scripts/ -v --tb=short || exit 1

      - name: Upload test results
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: backend/tests/scripts/

  build-and-push:
    runs-on: ubuntu-latest
    needs: test

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          platforms: linux/amd64
          push: true
          tags: |
            ${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest
            ${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          outputs: type=docker

      # 自动部署到 Sealos
      - name: Deploy to Sealos
        run: |
          mkdir -p ~/.kube
          # 兼容两种 Secret 存法：
          # 1) 直接存 kubeconfig YAML（推荐）
          # 2) 存 base64 后的 kubeconfig（需确保是未换行的单行 base64）
          KUBE_CONFIG_CONTENT='${{ secrets.KUBE_CONFIG }}'
          if printf '%s' "$KUBE_CONFIG_CONTENT" | tr -d '\r' | base64 -d > ~/.kube/config 2>/dev/null; then
            echo "kubeconfig loaded from base64 secret"
          else
            printf '%s' "$KUBE_CONFIG_CONTENT" | tr -d '\r' > ~/.kube/config
            echo "kubeconfig loaded from raw yaml secret"
          fi
          WORKLOAD_KIND="${K8S_WORKLOAD_KIND}"
          WORKLOAD_NAME="${K8S_WORKLOAD_NAME}"
          NAMESPACE="${K8S_NAMESPACE}"
          IMAGE_REF="${{ secrets.DOCKERHUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest"
          echo "Deploy context: $(kubectl config current-context)"
          echo "Deploy namespace: ${NAMESPACE}"

          if [ "$WORKLOAD_KIND" = "auto" ]; then
            if kubectl get deployment/"$WORKLOAD_NAME" -n "$NAMESPACE" >/dev/null 2>&1; then
              WORKLOAD_KIND="deployment"
            elif kubectl get statefulset/"$WORKLOAD_NAME" -n "$NAMESPACE" >/dev/null 2>&1; then
              WORKLOAD_KIND="statefulset"
            else
              echo "::error::No deployment/statefulset named '$WORKLOAD_NAME' in namespace '$NAMESPACE'"
              echo "Available workloads:"
              kubectl get deploy,sts -n "$NAMESPACE" || true
              exit 1
            fi
          fi

          if [ "$WORKLOAD_KIND" != "deployment" ] && [ "$WORKLOAD_KIND" != "statefulset" ]; then
            echo "::error::K8S_WORKLOAD_KIND must be 'deployment', 'statefulset', or 'auto'"
            exit 1
          fi

          CONTAINER_NAME="${K8S_CONTAINER_NAME}"
          if [ -z "$CONTAINER_NAME" ]; then
            CONTAINER_NAME=$(kubectl get "$WORKLOAD_KIND"/"$WORKLOAD_NAME" -n "$NAMESPACE" -o jsonpath='{.spec.template.spec.containers[0].name}')
          fi
          if [ -z "$CONTAINER_NAME" ]; then
            echo "::error::Cannot resolve container name for $WORKLOAD_KIND/$WORKLOAD_NAME"
            exit 1
          fi

          if [ "${REQUIRE_DATA_PVC}" = "true" ]; then
            echo "Checking /data persistence guard..."
            MOUNT_ROWS=$(kubectl get "$WORKLOAD_KIND"/"$WORKLOAD_NAME" -n "$NAMESPACE" \
              -o jsonpath="{range .spec.template.spec.containers[?(@.name=='$CONTAINER_NAME')].volumeMounts[*]}{.name}:{.mountPath}{'\n'}{end}")

            DATA_VOLUME_NAME=$(printf '%s\n' "$MOUNT_ROWS" | awk -F: '$2=="/data" || $2=="/data/app.db"{print $1; exit}')
            if [ -z "$DATA_VOLUME_NAME" ]; then
              echo "::error::Missing /data volumeMount for $WORKLOAD_KIND/$WORKLOAD_NAME container=$CONTAINER_NAME. Refusing deploy to prevent sqlite data loss."
              echo "Current volumeMounts:"
              printf '%s\n' "$MOUNT_ROWS"
              exit 1
            fi

            PVC_NAME=$(kubectl get "$WORKLOAD_KIND"/"$WORKLOAD_NAME" -n "$NAMESPACE" \
              -o jsonpath="{.spec.template.spec.volumes[?(@.name=='$DATA_VOLUME_NAME')].persistentVolumeClaim.claimName}")

            # StatefulSet commonly uses volumeClaimTemplates instead of .spec.template.spec.volumes.
            if [ -z "$PVC_NAME" ] && [ "$WORKLOAD_KIND" = "statefulset" ]; then
              TEMPLATE_NAME=$(kubectl get statefulset/"$WORKLOAD_NAME" -n "$NAMESPACE" \
                -o jsonpath="{.spec.volumeClaimTemplates[?(@.metadata.name=='$DATA_VOLUME_NAME')].metadata.name}")
              if [ -n "$TEMPLATE_NAME" ]; then
                PVC_NAME=$(kubectl get pvc -n "$NAMESPACE" \
                  -o jsonpath='{range .items[*]}{.metadata.name}{"\n"}{end}' \
                  | grep -E "^${TEMPLATE_NAME}-${WORKLOAD_NAME}-[0-9]+$" | head -n 1 || true)
              fi
            fi

            if [ -z "$PVC_NAME" ]; then
              echo "::error::Volume '$DATA_VOLUME_NAME' is not backed by persistentVolumeClaim. Refusing deploy."
              kubectl get "$WORKLOAD_KIND"/"$WORKLOAD_NAME" -n "$NAMESPACE" -o yaml | sed -n '1,220p'
              exit 1
            fi

            PVC_PHASE=$(kubectl get pvc "$PVC_NAME" -n "$NAMESPACE" -o jsonpath='{.status.phase}')
            if [ "$PVC_PHASE" != "Bound" ]; then
              echo "::error::PVC '$PVC_NAME' phase=$PVC_PHASE (expected Bound). Refusing deploy."
              kubectl get pvc "$PVC_NAME" -n "$NAMESPACE" -o yaml | sed -n '1,220p'
              exit 1
            fi

            echo "PVC guard passed: volume=$DATA_VOLUME_NAME pvc=$PVC_NAME phase=$PVC_PHASE"
          else
            echo "Skipping /data PVC guard (REQUIRE_DATA_PVC=${REQUIRE_DATA_PVC})"
          fi

          kubectl set image "$WORKLOAD_KIND"/"$WORKLOAD_NAME" "$CONTAINER_NAME"="$IMAGE_REF" -n "$NAMESPACE"

          # Run Alembic migrations via a one-off Job using the same image and the same /data PVC.
          # This avoids manual intervention and keeps DB schema in sync with code.
          MIGRATE_JOB="${WORKLOAD_NAME}-migrate-${GITHUB_SHA}"
          echo "Running DB migrations via Job: $MIGRATE_JOB"

          if [ -z "$PVC_NAME" ]; then
            echo "::error::Cannot resolve PVC_NAME for /data; refusing to run migrations."
            exit 1
          fi

          cat <<YAML | kubectl apply -n "$NAMESPACE" -f -
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: ${MIGRATE_JOB}
            labels:
              app.kubernetes.io/name: ${WORKLOAD_NAME}
              app.kubernetes.io/component: migrate
          spec:
            backoffLimit: 0
            ttlSecondsAfterFinished: 600
            template:
              spec:
                restartPolicy: Never
                containers:
                  - name: migrate
                    image: ${IMAGE_REF}
                    imagePullPolicy: Always
                    command: ["sh", "-lc", "python scripts/db_migrate.py upgrade"]
                    env:
                      - name: DATABASE_URL
                        value: "${DATABASE_URL:-sqlite:////data/app.db}"
                    volumeMounts:
                      - name: data
                        mountPath: /data
                volumes:
                  - name: data
                    persistentVolumeClaim:
                      claimName: ${PVC_NAME}
          YAML

          kubectl wait --for=condition=complete job/"$MIGRATE_JOB" -n "$NAMESPACE" --timeout=600s
          kubectl logs job/"$MIGRATE_JOB" -n "$NAMESPACE" || true

          echo "Restarting workload after migrations..."
          kubectl rollout restart "$WORKLOAD_KIND"/"$WORKLOAD_NAME" -n "$NAMESPACE"
          kubectl rollout status "$WORKLOAD_KIND"/"$WORKLOAD_NAME" -n "$NAMESPACE" --timeout=180s
